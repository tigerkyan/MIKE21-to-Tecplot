#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
MIKE21 to Tecplot è½¬æ¢å™¨
ä¸»ç¨‹åºç±»ï¼Œæä¾›å®Œæ•´çš„è½¬æ¢åŠŸèƒ½

ä½œè€…: GitHub Copilot
ç‰ˆæœ¬: 2.2 - æ·»åŠ çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†æ”¯æŒ
"""

import logging
import sys
from pathlib import Path
from typing import Dict, List, Optional, Union, Tuple
import numpy as np
import mikeio
from shapely.geometry import Point, Polygon, LineString
import ezdxf
# ä½¿ç”¨çº¿ç¨‹æ± æ›¿ä»£è¿›ç¨‹æ± ï¼Œé¿å…PyInstallerç¯å¢ƒé—®é¢˜
from concurrent.futures import ThreadPoolExecutor, as_completed
import yaml
import os
import threading


class MIKE21Converter:
    """MIKE21 DFSU æ–‡ä»¶åˆ° Tecplot æ ¼å¼è½¬æ¢å™¨"""

    def __init__(self, config_path: Optional[str] = None):
        """
        åˆå§‹åŒ–è½¬æ¢å™¨

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º config.yaml
        """
        self.config_path = config_path or "config.yaml"
        self.config = self._load_config()
        self._setup_logging()
        self.logger = logging.getLogger(__name__)

    def _load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ {self.config_path} æœªæ‰¾åˆ°")
        except yaml.YAMLError as e:
            raise ValueError(f"é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        log_level = logging.INFO if self.config.get('processing', {}).get('verbose', True) else logging.WARNING
        logging.basicConfig(
            level=log_level,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(sys.stdout),
                logging.FileHandler('mike21_converter.log', encoding='utf-8')
            ]
        )

    def load_closed_polyline(self, dxf_path: Path) -> Polygon:
        """ä»DXFæ–‡ä»¶åŠ è½½é—­åˆå¤šæ®µçº¿"""
        try:
            doc = ezdxf.readfile(dxf_path)
            pl = next((e for e in doc.modelspace().query("LWPOLYLINE") if e.closed), None)
            if pl is None:
                raise ValueError("DXF ä¸­æ‰¾ä¸åˆ°é—­åˆå¤šæ®µçº¿ï¼è¯·ç¡®è®¤å·²æ‰§è¡Œ C é—­åˆã€‚")
            coords = [(x, y) for x, y, *_ in pl.get_points("xy")]
            return Polygon(coords)
        except Exception as e:
            self.logger.error(f"åŠ è½½DXFæ–‡ä»¶ {dxf_path} å¤±è´¥: {e}")
            raise

    def load_axis_polyline(self, dxf_path: Path) -> LineString:
        """ä»DXFæ–‡ä»¶åŠ è½½è½´çº¿"""
        try:
            doc = ezdxf.readfile(dxf_path)
            line = next((e for e in doc.modelspace().query("LWPOLYLINE")), None)
            if line is None:
                raise ValueError("DXF ä¸­æ‰¾ä¸åˆ°å¤šæ®µçº¿")
            coords = [(x, y) for x, y, *_ in line.get_points("xy")]
            return LineString(coords)
        except Exception as e:
            self.logger.error(f"åŠ è½½è½´çº¿æ–‡ä»¶ {dxf_path} å¤±è´¥: {e}")
            raise

    def project_uv_along_axis(self, elem_xy: np.ndarray, u: np.ndarray,
                             v: np.ndarray, axis: LineString) -> Tuple[np.ndarray, np.ndarray]:
        """å°†é€Ÿåº¦çŸ¢é‡æŠ•å½±åˆ°è½´çº¿åæ ‡ç³»"""
        vx, vy = [], []
        for xyz, uu, vv in zip(elem_xy, u, v):
            x, y = xyz[:2]
            pt = Point(x, y)
            proj = axis.project(pt)
            axis_pt = axis.interpolate(proj)
            tangent = np.array(axis.interpolate(proj + 1e-6).coords[0]) - np.array(axis_pt.coords[0])
            if np.linalg.norm(tangent) == 0:
                tangent = np.array([1.0, 0.0])
            else:
                tangent = tangent / np.linalg.norm(tangent)
            normal = np.array([-tangent[1], tangent[0]])
            vel_vec = np.array([uu, vv])
            vx.append(np.dot(vel_vec, tangent))
            vy.append(np.dot(vel_vec, normal))
        return np.array(vx), np.array(vy)

    def write_tecplot_elements(self, out_path: Path, elem_xy: np.ndarray,
                              variables: np.ndarray, title: str = "MIKE21 Data"):
        """è¾“å‡ºå•å…ƒä¸­å¿ƒæ•°æ®åˆ°Tecplotæ ¼å¼"""
        precision = self.config.get('output_settings', {}).get('precision', 6)
        variables = np.nan_to_num(variables, nan=0.0)

        with open(out_path, "w", encoding='utf-8') as f:
            f.write(f'TITLE = "{title}"\n')
            var_names = ["X", "Y", "u", "v", "w", "velocity"]
            if variables.shape[1] > 6:
                var_names.extend(["Vx", "Vy"])
            f.write('VARIABLES = ' + ', '.join(f'"{v}"' for v in var_names) + '\n')
            f.write(f'ZONE I={len(elem_xy)}, DATAPACKING=POINT\n')
            for row in variables:
                f.write(" ".join(f"{v:.{precision}f}" for v in row) + "\n")

    def write_tecplot_nodes(self, out_path: Path, node_xy: np.ndarray,
                           conn_reindex: np.ndarray, variables: np.ndarray,
                           title: str = "MIKE21 Data"):
        """è¾“å‡ºèŠ‚ç‚¹æ•°æ®åˆ°Tecplotæ ¼å¼"""
        precision = self.config.get('output_settings', {}).get('precision', 6)
        variables = np.nan_to_num(variables, nan=0.0)

        with open(out_path, "w", encoding='utf-8') as f:
            f.write(f'TITLE = "{title}"\n')
            var_names = ["X", "Y", "u", "v", "w", "velocity"]
            if variables.shape[1] > 6:
                var_names.extend(["Vx", "Vy"])
            f.write('VARIABLES = ' + ', '.join(f'"{v}"' for v in var_names) + '\n')
            f.write(f'ZONE N={len(node_xy)}, E={len(conn_reindex)}, F=FEPOINT, ET=TRIANGLE\n')
            for row in variables:
                f.write(" ".join(f"{v:.{precision}f}" for v in row) + "\n")
            for tri in conn_reindex:
                f.write(f"{tri[0]+1} {tri[1]+1} {tri[2]+1}\n")

    def process_full_field(self, ds, dfsu_path: Path, out_dir: Path) -> bool:
        """å¤„ç†å…¨åœºæ•°æ®è¾“å‡º"""
        if not self.config.get('output_settings', {}).get('export_full_field', True):
            return False

        try:
            # è¯»å–é€Ÿåº¦æ•°æ®
            u = ds["U velocity"].values
            v = ds["V velocity"].values
            w = ds["W velocity"].values if "W velocity" in ds.items else np.zeros_like(u)
            velocity = np.sqrt(u**2 + v**2 + w**2)

            # è·å–å‡ ä½•ä¿¡æ¯
            node_xy_all = np.array(ds.geometry.node_coordinates)
            elem_xy = np.array(ds.geometry.element_coordinates)
            elem_tab = np.array([np.array(e, dtype=int) for e in ds.geometry.element_table if len(e) == 3])

            # åæ ‡å˜æ¢å‚æ•°
            x_shift = self.config.get('coordinate_transform', {}).get('x_shift', 0)
            y_shift = self.config.get('coordinate_transform', {}).get('y_shift', 0)

            if u.shape[0] == elem_xy.shape[0]:
                # å•å…ƒä¸­å¿ƒæ•°æ®
                out_all = out_dir / f"{dfsu_path.stem}_allfield.dat"
                elem_xy_out = elem_xy.copy()
                elem_xy_out[:, 0] -= x_shift
                elem_xy_out[:, 1] -= y_shift
                vars_all = np.column_stack([elem_xy_out[:, 0], elem_xy_out[:, 1], u, v, w, velocity])
                self.write_tecplot_elements(out_all, elem_xy_out, vars_all,
                                          "MIKE21 å…¨åœºæµé€ŸçŸ¢é‡(å•å…ƒä¸­å¿ƒ)")
                self.logger.info(f"âœ… å…¨åœºè¾“å‡º(å•å…ƒä¸­å¿ƒ): {out_all.name}, æ•°æ®ç‚¹æ•°: {len(elem_xy_out)}")

            elif u.shape[0] == node_xy_all.shape[0]:
                # èŠ‚ç‚¹æ•°æ®
                out_all = out_dir / f"{dfsu_path.stem}_allfield.dat"
                node_xy_all_out = node_xy_all.copy()
                node_xy_all_out[:, 0] -= x_shift
                node_xy_all_out[:, 1] -= y_shift
                vars_all = np.column_stack([node_xy_all_out[:, 0], node_xy_all_out[:, 1], u, v, w, velocity])
                self.write_tecplot_nodes(out_all, node_xy_all_out, elem_tab, vars_all,
                                       "MIKE21 å…¨åœºæµé€ŸçŸ¢é‡(èŠ‚ç‚¹)")
                self.logger.info(f"âœ… å…¨åœºè¾“å‡º(èŠ‚ç‚¹): {out_all.name}, èŠ‚ç‚¹æ•°: {len(node_xy_all_out)}, å•å…ƒæ•°: {len(elem_tab)}")
            else:
                raise ValueError(f"æ•°æ®ç»´åº¦ä¸åŒ¹é…: èŠ‚ç‚¹æ•°{node_xy_all.shape[0]}, å•å…ƒæ•°{elem_xy.shape[0]}, é€Ÿåº¦åœºé•¿åº¦{u.shape[0]}")

            return True

        except Exception as e:
            self.logger.error(f"å…¨åœºå¤„ç†å¤±è´¥: {e}")
            return False

    def process_regions(self, ds, dfsu_path: Path, out_dir: Path) -> Dict[str, bool]:
        """å¤„ç†åŒºåŸŸæ•°æ®è¾“å‡º"""
        if not self.config.get('output_settings', {}).get('export_regions', True):
            return {}

        results = {}
        regions = self.config.get('regions', {})

        # è¯»å–æ•°æ®
        u = ds["U velocity"].values
        v = ds["V velocity"].values
        w = ds["W velocity"].values if "W velocity" in ds.items else np.zeros_like(u)
        velocity = np.sqrt(u**2 + v**2 + w**2)

        node_xy_all = np.array(ds.geometry.node_coordinates)
        elem_xy = np.array(ds.geometry.element_coordinates)
        elem_tab = np.array([np.array(e, dtype=int) for e in ds.geometry.element_table if len(e) == 3])

        x_shift = self.config.get('coordinate_transform', {}).get('x_shift', 0)
        y_shift = self.config.get('coordinate_transform', {}).get('y_shift', 0)

        for name, region_config in regions.items():
            try:
                # åŠ è½½åŒºåŸŸå’Œè½´çº¿
                region_poly = self.load_closed_polyline(Path(region_config["region_dxf"]))
                axis_line = self.load_axis_polyline(Path(region_config["axis_dxf"]))

                # ç­›é€‰åŒºåŸŸå†…çš„å•å…ƒ
                mask_elem = np.array([region_poly.contains(Point(xy[0], xy[1])) for xy in elem_xy])
                if not mask_elem.any():
                    self.logger.warning(f"âš ï¸ åŒºåŸŸ {name} åœ¨ {dfsu_path.name} ä¸­æ— å…ƒç´ ï¼Œå·²è·³è¿‡è¯¥åŒºåŸŸã€‚")
                    results[name] = False
                    continue

                # æå–åŒºåŸŸæ•°æ®
                u_r = u[mask_elem]
                v_r = v[mask_elem]
                w_r = w[mask_elem]
                vel_r = velocity[mask_elem]
                elem_tab_r = elem_tab[mask_elem]
                elem_xy_r = elem_xy[mask_elem]

                # æŠ•å½±åˆ°è½´çº¿åæ ‡ç³»
                vx_r, vy_r = self.project_uv_along_axis(elem_xy_r, u_r, v_r, axis_line)

                # é‡å»ºè¿æ¥è¡¨
                nodes_keep = np.unique(elem_tab_r)
                node_map = {old: new for new, old in enumerate(nodes_keep)}
                conn_reindex = np.vectorize(node_map.get)(elem_tab_r)
                node_xy = node_xy_all[nodes_keep]

                # æ„å»ºè¾“å‡ºå˜é‡
                vars_region = []
                for i in range(len(node_xy)):
                    xi, yi = node_xy[i][:2]
                    xi -= x_shift
                    yi -= y_shift
                    idx_match = np.any(elem_tab_r == nodes_keep[i], axis=1)
                    u_m = u_r[idx_match].mean()
                    v_m = v_r[idx_match].mean()
                    w_m = w_r[idx_match].mean()
                    vel_m = vel_r[idx_match].mean()
                    vx_m = vx_r[idx_match].mean()
                    vy_m = vy_r[idx_match].mean()
                    vars_region.append([xi, yi, u_m, v_m, w_m, vel_m, vx_m, vy_m])

                # è¾“å‡ºæ–‡ä»¶
                out_region = out_dir / f"{dfsu_path.stem}_{name}.dat"
                description = region_config.get('description', name)
                self.write_tecplot_nodes(out_region, node_xy, conn_reindex,
                                       np.array(vars_region), f"MIKE21 åŒºåŸŸ: {description}")
                self.logger.info(f"âœ… åŒºåŸŸ {name} è¾“å‡º: {out_region.name}")
                results[name] = True

            except Exception as e:
                self.logger.error(f"âŒ åŒºåŸŸ {name} å¤„ç†å¤±è´¥: {e}")
                results[name] = False

        return results

    def process_single_file(self, dfsu_path: Path) -> Dict:
        """å¤„ç†å•ä¸ªDFSUæ–‡ä»¶"""
        self.logger.info(f"ğŸ“‚ å¤„ç†æ–‡ä»¶: {dfsu_path.name}")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(self.config['paths']['output_dir'])
        out_dir = output_dir / dfsu_path.stem
        out_dir.mkdir(parents=True, exist_ok=True)

        try:
            # è¯»å–DFSUæ–‡ä»¶
            dfs = mikeio.open(dfsu_path)
            time_index = self.config.get('time_settings', {}).get('time_index')

            if time_index is None:
                ds = dfs.read()
            else:
                ds = dfs.read(time=[time_index])
                if ds.n_timesteps == 1:
                    ds = ds.isel(time=0)

            # å¤„ç†å…¨åœºå’ŒåŒºåŸŸæ•°æ®
            full_field_success = self.process_full_field(ds, dfsu_path, out_dir)
            region_results = self.process_regions(ds, dfsu_path, out_dir)

            self.logger.info(f"âœ… å®Œæˆ: {dfsu_path.name}")

            return {
                'file': dfsu_path.name,
                'success': True,
                'full_field': full_field_success,
                'regions': region_results
            }

        except Exception as e:
            self.logger.error(f"âŒ å¤„ç† {dfsu_path.name} æ—¶å‡ºé”™: {e}")
            return {
                'file': dfsu_path.name,
                'success': False,
                'error': str(e)
            }

    def run(self, input_files: Optional[List[str]] = None) -> Dict:
        """è¿è¡Œè½¬æ¢å™¨ - æ”¯æŒçº¿ç¨‹æ± å¹¶è¡Œå¤„ç†"""
        # ç¡®å®šè¾“å…¥æ–‡ä»¶
        if input_files:
            dfsu_files = [Path(f) for f in input_files if Path(f).exists()]
        else:
            input_dir = Path(self.config['paths']['input_dir'])
            dfsu_files = list(input_dir.glob("*.dfsu"))

        if not dfsu_files:
            self.logger.error("æœªæ‰¾åˆ°ä»»ä½•DFSUæ–‡ä»¶ï¼")
            return {'success': False, 'message': 'æœªæ‰¾åˆ°ä»»ä½•DFSUæ–‡ä»¶'}

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = Path(self.config['paths']['output_dir'])
        output_dir.mkdir(exist_ok=True)

        # å¹¶è¡Œå¤„ç†é…ç½®
        max_workers = self.config.get('processing', {}).get('parallel_workers')
        if max_workers is None:
            max_workers = min(len(dfsu_files), os.cpu_count() or 1)

        # åœ¨PyInstallerç¯å¢ƒä¸­ä½¿ç”¨çº¿ç¨‹æ± è€Œä¸æ˜¯è¿›ç¨‹æ± 
        use_parallel = self.config.get('processing', {}).get('enable_parallel', True)

        # å¦‚æœåªæœ‰ä¸€ä¸ªæ–‡ä»¶æˆ–é…ç½®ä¸ºç¦ç”¨å¹¶è¡Œå¤„ç†ï¼Œåˆ™ä½¿ç”¨å•çº¿ç¨‹
        if len(dfsu_files) == 1 or max_workers == 1 or not use_parallel:
            self.logger.info(f"å¼€å§‹å¤„ç† {len(dfsu_files)} ä¸ªæ–‡ä»¶ï¼ˆå•çº¿ç¨‹æ¨¡å¼ï¼‰")
            results = []
            for dfsu in dfsu_files:
                results.append(self.process_single_file(dfsu))
        else:
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†å¤šä¸ªæ–‡ä»¶
            self.logger.info(f"å¼€å§‹å¤„ç† {len(dfsu_files)} ä¸ªæ–‡ä»¶ï¼Œä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹")
            results = []

            # åˆ›å»ºçº¿ç¨‹é”æ¥ä¿æŠ¤æ—¥å¿—è¾“å‡º
            log_lock = threading.Lock()

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_file = {
                    executor.submit(self._process_file_with_lock, dfsu, log_lock): dfsu
                    for dfsu in dfsu_files
                }

                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_file):
                    try:
                        result = future.result()
                        results.append(result)

                        # å®‰å…¨åœ°è¾“å‡ºè¿›åº¦ä¿¡æ¯
                        with log_lock:
                            if result['success']:
                                self.logger.info(f"ğŸ¯ å®Œæˆæ–‡ä»¶: {result['file']}")
                            else:
                                self.logger.error(f"âŒ å¤±è´¥æ–‡ä»¶: {result['file']}")

                    except Exception as e:
                        file_path = future_to_file[future]
                        with log_lock:
                            self.logger.error(f"âŒ çº¿ç¨‹å¤„ç† {file_path.name} æ—¶å‡ºé”™: {e}")
                        results.append({
                            'file': file_path.name,
                            'success': False,
                            'error': str(e)
                        })

        # æ±‡æ€»ç»“æœ
        successful = sum(1 for r in results if r['success'])
        processing_mode = "å¹¶è¡Œ" if len(dfsu_files) > 1 and max_workers > 1 and use_parallel else "å•çº¿ç¨‹"
        self.logger.info(f"å¤„ç†å®Œæˆï¼ˆ{processing_mode}æ¨¡å¼ï¼‰: {successful}/{len(dfsu_files)} ä¸ªæ–‡ä»¶æˆåŠŸ")

        return {
            'success': True,
            'total_files': len(dfsu_files),
            'successful_files': successful,
            'processing_mode': processing_mode,
            'max_workers': max_workers if processing_mode == "å¹¶è¡Œ" else 1,
            'results': results
        }

    def _process_file_with_lock(self, dfsu_path: Path, log_lock: threading.Lock) -> Dict:
        """å¸¦çº¿ç¨‹é”çš„æ–‡ä»¶å¤„ç†æ–¹æ³•ï¼Œç¡®ä¿æ—¥å¿—è¾“å‡ºçš„çº¿ç¨‹å®‰å…¨"""
        try:
            # åœ¨å¼€å§‹å¤„ç†æ—¶å®‰å…¨åœ°è¾“å‡ºæ—¥å¿—
            with log_lock:
                self.logger.info(f"ğŸš€ å¼€å§‹å¤„ç†: {dfsu_path.name} [çº¿ç¨‹: {threading.current_thread().name}]")

            # è°ƒç”¨åŸå§‹çš„å¤„ç†æ–¹æ³•
            return self.process_single_file(dfsu_path)

        except Exception as e:
            with log_lock:
                self.logger.error(f"âŒ æ–‡ä»¶å¤„ç†å¼‚å¸¸ {dfsu_path.name}: {e}")
            return {
                'file': dfsu_path.name,
                'success': False,
                'error': str(e)
            }
def main():
    """ä¸»å‡½æ•°"""
    try:
        converter = MIKE21Converter()
        result = converter.run()

        if result['success']:
            print(f"\nè½¬æ¢å®Œæˆï¼æˆåŠŸå¤„ç† {result['successful_files']}/{result['total_files']} ä¸ªæ–‡ä»¶")
        else:
            print(f"\nè½¬æ¢å¤±è´¥ï¼š{result.get('message', 'æœªçŸ¥é”™è¯¯')}")

    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()